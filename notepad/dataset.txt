import torch
import os
import cv2
import pandas as pd
import numpy as np
from skimage import io
import config
import utils
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

resize = 1024

def train_test_split(csv_path, split):
    df_data = pd.read_csv(csv_path)
    # drop all the rows with missing values
    df_data = df_data.dropna()
    len_data = len(df_data)
    # calculate the validation data sample length
    valid_split = int(len_data * split)
    # calculate the training data samples length
    train_split = int(len_data - valid_split)
    training_samples = df_data.iloc[:train_split][:]
    valid_samples = df_data.iloc[-valid_split:][:]
    print(f"Training sample instances: {len(training_samples)}")
    print(f"Validation sample instances: {len(valid_samples)}")
    return training_samples, valid_samples


class AsteroidDataset(Dataset):
    
    csv_file = 'C:\\Users\\ak234\\OneDrive\\Documents\\Year 4 Uni\\Dissertation\\1513 images\\input\\asteroid com detection\\training\\training.csv'
    root_dir = 'C:\\Users\\ak234\\OneDrive\\Documents\\Year 4 Uni\\Dissertation\\1513 images\\input\\asteroid com detection\\training\\training and validation images'
    
    def __init__(self, csv_file, root_dir):
       
        self.data = pd.read_csv(csv_file)
        self.root_dir = root_dir
        
        img_name = os.path.join(self.root_dir,
                                self.data.iloc[-1, 0])
        
        image = io.imread(img_name)
      
        self.pixel_col = image.reshape(1024*1024)
        
        self.image_pixels = []
        for i in tqdm(range(len(self.data))):
            for j in range(len(self.pixel_col)):
                
                img = self.pixel_col[j]
          
            self.image_pixels.append(img)
        print(len(self.image_pixels))
        #self.images = np.array(image.reshape(1024*1024, 1), dtype='float32')
        self.images = np.array(self.image_pixels, dtype='float32')
    def __len__(self):
        
        return len(self.images)
    
    def __getitem__(self, index):
        
        image = self.images[index].reshape(1024, 1024)
        print(image.shape)
        
        orig_w, orig_h = image.shape
        # resize the image into `resize` defined above
        image = cv2.resize(image, (resize, resize))
        # again reshape to add grayscale channel format
        #image = image.reshape(resize, resize, 1)
        image = image / 255.0
        # transpose for getting the channel size to index 0
        image = np.transpose(image, (2, 0, 1))
        # get the keypoints
        keypoints = self.data.iloc[index][1:]
        keypoints = np.array(keypoints, dtype='float32')
        # reshape the keypoints
        keypoints = keypoints.reshape(-1, 2)
        # rescale keypoints according to image resize
        keypoints = keypoints * [resize / orig_w, resize / orig_h]
        return {
            'image': torch.tensor(image, dtype=torch.float),
            'keypoints': torch.tensor(keypoints, dtype=torch.float),
        }
    

    
# get the training and validation data samples
#training_samples, valid_samples = train_test_split(f"{config.ROOT_PATH}\\training\\training.csv",
                                               #config.TEST_SPLIT)


# initialize the dataset - `AsteroidDataset()`
print('\n-------------- PREPARING DATA --------------\n')
train_data = AsteroidDataset(csv_file='C:\\Users\\ak234\\OneDrive\\Documents\\Year 4 Uni\\Dissertation\\1513 images\\input\\asteroid com detection\\training\\training.csv' , root_dir='C:\\Users\\ak234\\OneDrive\\Documents\\Year 4 Uni\\Dissertation\\1513 images\\input\\asteroid com detection\\training\\training and validation images' )
valid_data = AsteroidDataset(csv_file='C:\\Users\\ak234\\OneDrive\\Documents\\Year 4 Uni\\Dissertation\\1513 images\\input\\asteroid com detection\\validation\\validation.csv' , root_dir='C:\\Users\\ak234\\OneDrive\\Documents\\Year 4 Uni\\Dissertation\\1513 images\\input\\asteroid com detection\\validation\\validation images' )
#valid_data = AsteroidDataset(valid_samples)
#print('\n-------------- DATA PREPRATION DONE --------------\n')
# prepare data loaders

train_loader = DataLoader(train_data, 
                          batch_size=config.BATCH_SIZE, 
                          shuffle=True)

valid_loader = DataLoader(valid_data, 
                          batch_size=config.BATCH_SIZE, 
                          shuffle=False)

# whether to show dataset keypoint plots
#if config.SHOW_DATASET_PLOT:
 #   utils.dataset_keypoints_plot(valid_data)